{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_data(file_path, output_dir):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    \n",
    "    # Resample data into 10-minute intervals\n",
    "    volume_resampled = data['volume'].resample('10T').sum()  # Sum volume\n",
    "    \n",
    "    # Calculate returns based on open price at the start and close price at the end of the interval\n",
    "    open_resampled = data['open'].resample('10T').first()\n",
    "    close_resampled = data['close'].resample('10T').last()\n",
    "    returns_resampled = (close_resampled - open_resampled) / open_resampled\n",
    "    \n",
    "    # Combine into a new DataFrame\n",
    "    resampled_data = pd.DataFrame({\n",
    "        'open': open_resampled,\n",
    "        'close': close_resampled,\n",
    "        'volume': volume_resampled,\n",
    "        'returns': returns_resampled\n",
    "    }).dropna()  # Drop any rows with NaN values which may arise from empty intervals\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract the company name from the file path and create the output file name\n",
    "    company_name = os.path.basename(file_path).replace('.csv', '')\n",
    "    output_file_path = os.path.join(output_dir, f'Updated_{company_name}.csv')\n",
    "    \n",
    "    # Save the resampled data to the new file\n",
    "    resampled_data.to_csv(output_file_path)\n",
    "    print(f\"Preprocessed data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to preprocessed_data/./Updated_SPY_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_XRAY_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_RHI_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_NCLH_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_CMA_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_MHK_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_BWA_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_VFC_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_ZION_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_CMA_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_ETSY_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_BWA_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_IVZ_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_XRAY_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_DOC_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_MKTX_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_FMC_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_PNW_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_VFC_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_HAS_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_FRT_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_PARA_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_CHRW_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_GNRC_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_PARA_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_MHK_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_IVZ_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_DOC_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_FMC_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_WHR_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_RHI_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_ETSY_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_CHRW_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_HAS_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_PNW_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_GNRC_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_MKTX_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_WHR_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_FRT_Data_enhanced.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_NCLH_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Bottom_20/Updated_ZION_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Mag_7/Updated_GOOG_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Mag_7/Updated_NVDA_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Mag_7/Updated_TSLA_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Mag_7/Updated_MSFT_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Mag_7/Updated_AAPL_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Mag_7/Updated_AMZN_Data.csv\n",
      "Preprocessed data saved to preprocessed_data/Mag_7/Updated_META_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_all_data(root_folder, output_root):\n",
    "    \"\"\"\n",
    "    Preprocess all stock data files in the given root folder and outputs them to a new structure under output_root.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, root_folder)\n",
    "                output_dir = os.path.join(output_root, relative_path)\n",
    "                \n",
    "                preprocess_stock_data(file_path, output_dir)\n",
    "\n",
    "# Define your original data folder and output folder\n",
    "root_folder = 'Data'  # Adjust if your root data folder has a different name or path\n",
    "output_root = 'preprocessed_data'\n",
    "\n",
    "preprocess_all_data(root_folder, output_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_timestamps(root_folder):\n",
    "    all_timestamps = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # Ensure timestamp is in a consistent format, if necessary\n",
    "                data['timestamp'] = pd.to_datetime(data['timestamp']).dt.floor('T')  # Floor to minute precision\n",
    "                file_timestamps = set(data['timestamp'].astype(str))  # Convert to string for easy comparison\n",
    "                \n",
    "                all_timestamps.append(file_timestamps)\n",
    "    \n",
    "    # Find the intersection of timestamps across all files\n",
    "    common_timestamps = set.intersection(*all_timestamps)\n",
    "    \n",
    "    return common_timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_datasets_to_common_timestamps(root_folder, common_timestamps):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                input_path = os.path.join(root, file)\n",
    "                data = pd.read_csv(input_path)\n",
    "                \n",
    "                data['timestamp'] = pd.to_datetime(data['timestamp']).dt.floor('T')\n",
    "                data['timestamp'] = data['timestamp'].astype(str)\n",
    "                \n",
    "                data_filtered = data[data['timestamp'].isin(common_timestamps)]\n",
    "                \n",
    "                # Save the filtered dataset back to its original location\n",
    "                data_filtered.to_csv(input_path, index=False)\n",
    "                print(f\"Filtered dataset updated at {input_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset updated at preprocessed_data/Updated_SPY_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_XRAY_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_FRT_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_MKTX_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_IVZ_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_PNW_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_VFC_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_DOC_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_BWA_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_RHI_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_ETSY_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_MHK_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_CMA_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_GNRC_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_NCLH_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_HAS_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_WHR_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_ZION_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_PARA_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_FMC_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Bottom_20/Updated_CHRW_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Mag_7/Updated_GOOG_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Mag_7/Updated_NVDA_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Mag_7/Updated_AMZN_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Mag_7/Updated_META_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Mag_7/Updated_TSLA_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Mag_7/Updated_MSFT_Data.csv\n",
      "Filtered dataset updated at preprocessed_data/Mag_7/Updated_AAPL_Data.csv\n"
     ]
    }
   ],
   "source": [
    "root_folder = 'preprocessed_data'\n",
    "\n",
    "# Collect common timestamps across all datasets\n",
    "common_timestamps = collect_timestamps(root_folder)\n",
    "\n",
    "# Filter each dataset to only include common timestamps and update them in-place\n",
    "filter_datasets_to_common_timestamps(root_folder, common_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mag7_spy_to_bottom20(mag7_folder, bottom20_folder, spy_file):\n",
    "    # Load SPY data\n",
    "    spy_data = pd.read_csv(spy_file, index_col='timestamp', parse_dates=['timestamp'])\n",
    "    \n",
    "    # Load Mag_7 data into a dictionary\n",
    "    mag7_data = {}\n",
    "    for filename in os.listdir(mag7_folder):\n",
    "        if filename.startswith(\"Updated_\") and filename.endswith(\"_Data.csv\"):\n",
    "            filepath = os.path.join(mag7_folder, filename)\n",
    "            # Extract company name considering the 'Updated_' prefix and '_Data.csv' suffix\n",
    "            company_name = filename.replace(\"Updated_\", \"\").replace(\"_Data.csv\", \"\")\n",
    "            mag7_data[company_name] = pd.read_csv(filepath, index_col='timestamp', parse_dates=['timestamp'])\n",
    "\n",
    "    # Iterate through Bottom_20 and add Mag_7 and SPY data\n",
    "    for bottom20_file in os.listdir(bottom20_folder):\n",
    "        if bottom20_file.startswith(\"Updated_\") and bottom20_file.endswith(\"_Data.csv\"):\n",
    "            bottom20_filepath = os.path.join(bottom20_folder, bottom20_file)\n",
    "            bottom20_data = pd.read_csv(bottom20_filepath, index_col='timestamp', parse_dates=['timestamp'])\n",
    "\n",
    "            # Add Mag_7 volume and returns\n",
    "            for company_name, df in mag7_data.items():\n",
    "                bottom20_data[f'{company_name} Volume'] = df['volume'].reindex(bottom20_data.index).fillna(0)\n",
    "                bottom20_data[f'{company_name} Returns'] = df['returns'].reindex(bottom20_data.index).fillna(0)\n",
    "\n",
    "            # Add SPY volume and returns\n",
    "            bottom20_data['SPY Volume'] = spy_data['volume'].reindex(bottom20_data.index).fillna(0)\n",
    "            bottom20_data['SPY Returns'] = spy_data['returns'].reindex(bottom20_data.index).fillna(0)\n",
    "\n",
    "            # Save the updated Bottom_20 data\n",
    "            bottom20_data.to_csv(bottom20_filepath)\n",
    "            print(f\"Updated {bottom20_file} with Mag_7 and SPY data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Updated_XRAY_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_FRT_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_MKTX_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_IVZ_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_PNW_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_VFC_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_DOC_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_BWA_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_RHI_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_ETSY_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_MHK_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_CMA_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_GNRC_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_NCLH_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_HAS_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_WHR_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_ZION_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_PARA_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_FMC_Data.csv with Mag_7 and SPY data.\n",
      "Updated Updated_CHRW_Data.csv with Mag_7 and SPY data.\n"
     ]
    }
   ],
   "source": [
    "mag7_folder = 'preprocessed_data/Mag_7'\n",
    "bottom20_folder = 'preprocessed_data/Bottom_20'\n",
    "spy_file = 'preprocessed_data/Updated_SPY_Data.csv'  # Adjust if your filename differs\n",
    "\n",
    "add_mag7_spy_to_bottom20(mag7_folder, bottom20_folder, spy_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_slices(file_path, start_date, end_date, train_minutes, test_minutes):\n",
    "    # Constants\n",
    "    interval_length = 10  # Each observation covers a 10-minute interval\n",
    "    mag7_companies = ['AAPL', 'AMZN', 'MSFT', 'TSLA', 'META', 'NVDA', 'GOOG']\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    \n",
    "    # Filter data by the specified date range\n",
    "    data = data[(data.index.date >= pd.to_datetime(start_date).date()) & \n",
    "                (data.index.date <= pd.to_datetime(end_date).date())]\n",
    "    \n",
    "    # Identify Mag_7 and SPY column names\n",
    "    mag7_columns = [col for col in data.columns if any(company in col for company in mag7_companies)]\n",
    "    spy_columns = [col for col in data.columns if 'SPY' in col]\n",
    "    \n",
    "    # Initialize lists for train and test slices\n",
    "    mag7_train_slices = []\n",
    "    spy_train_slices = []\n",
    "    test_slices = []\n",
    "    \n",
    "    # Calculate the number of intervals for training and testing\n",
    "    train_intervals = train_minutes // interval_length\n",
    "    test_intervals = test_minutes // interval_length\n",
    "    \n",
    "    start_idx = 0\n",
    "    total_intervals = len(data)\n",
    "    \n",
    "    # Loop to slice the data for training and testing intervals\n",
    "    while start_idx + train_intervals + test_intervals <= total_intervals:\n",
    "        # Training and testing indices\n",
    "        end_train_idx = start_idx + train_intervals\n",
    "        start_test_idx = end_train_idx\n",
    "        end_test_idx = start_test_idx + test_intervals\n",
    "        \n",
    "        # Append train slices\n",
    "        mag7_train_slices.append(data.iloc[start_idx:end_train_idx][mag7_columns])\n",
    "        spy_train_slices.append(data.iloc[start_idx:end_train_idx][spy_columns])\n",
    "        \n",
    "        # Append test slice (only once per cycle since it's the same for both train sets)\n",
    "        test_slices.append(data.iloc[start_test_idx:end_test_idx])\n",
    "        \n",
    "        # Update the start index for the next slicing cycle\n",
    "        start_idx = end_test_idx\n",
    "    \n",
    "    return mag7_train_slices, spy_train_slices, test_slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def train_and_evaluate(train_slice_mag7, train_slice_spy, test_slice):\n",
    "    # Prepare target and features for training with Mag7\n",
    "    y_train = train_slice_mag7['returns']\n",
    "    X_train_mag7 = train_slice_mag7.drop(columns=['returns'])\n",
    "\n",
    "    # Prepare features for training with SPY\n",
    "    X_train_spy = train_slice_spy.drop(columns=['returns'])\n",
    "\n",
    "    # Prepare target and features for testing\n",
    "    y_test = test_slice['returns']\n",
    "    X_test = test_slice.drop(columns=['returns'])\n",
    "\n",
    "    # Train model on Mag7 data\n",
    "    model_mag7 = LinearRegression()\n",
    "    model_mag7.fit(X_train_mag7, y_train)\n",
    "    predictions_mag7 = model_mag7.predict(X_test)\n",
    "\n",
    "    # Train model on SPY data\n",
    "    model_spy = LinearRegression()\n",
    "    model_spy.fit(X_train_spy, y_train)\n",
    "    predictions_spy = model_spy.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE for predictions\n",
    "    rmse_mag7 = sqrt(mean_squared_error(y_test, predictions_mag7))\n",
    "    rmse_spy = sqrt(mean_squared_error(y_test, predictions_spy))\n",
    "\n",
    "    return rmse_mag7, rmse_spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'returns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3800'>3801</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3801'>3802</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3802'>3803</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'returns'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m rmse_scores_spy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_slices)):\n\u001b[0;32m---> 17\u001b[0m     rmse_mag7, rmse_spy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmag7_train_slices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspy_train_slices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_slices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     rmse_scores_mag7\u001b[38;5;241m.\u001b[39mappend(rmse_mag7)\n\u001b[1;32m     19\u001b[0m     rmse_scores_spy\u001b[38;5;241m.\u001b[39mappend(rmse_spy)\n",
      "Cell \u001b[0;32mIn[39], line 12\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_slice_mag7, train_slice_spy, test_slice)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate\u001b[39m(train_slice_mag7, train_slice_spy, test_slice):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Prepare target and features for training with Mag7\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_slice_mag7\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     X_train_mag7 \u001b[38;5;241m=\u001b[39m train_slice_mag7\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Prepare features for training with SPY\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/frame.py?line=3804'>3805</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/frame.py?line=3805'>3806</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/frame.py?line=3806'>3807</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/frame.py?line=3807'>3808</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/frame.py?line=3808'>3809</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3801'>3802</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3802'>3803</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3803'>3804</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3804'>3805</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3805'>3806</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3806'>3807</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3807'>3808</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/smallina/opt/anaconda3/envs/cse185/lib/python3.10/site-packages/pandas/core/indexes/base.py?line=3808'>3809</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'returns'"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = 'preprocessed_data/Bottom_20/Updated_MKTX_Data.csv'\n",
    "start_date = '2023-09-01'\n",
    "end_date = '2023-09-02'\n",
    "train_minutes = 180\n",
    "test_minutes = 30\n",
    "\n",
    "\n",
    "mag7_train_slices, spy_train_slices, test_slices = generate_train_test_slices(\n",
    "    file_path, start_date, end_date, train_minutes, test_minutes\n",
    ")\n",
    "\n",
    "# Evaluate models and collect RMSE scores\n",
    "rmse_scores_mag7 = []\n",
    "rmse_scores_spy = []\n",
    "\n",
    "for i in range(len(test_slices)):\n",
    "    rmse_mag7, rmse_spy = train_and_evaluate(mag7_train_slices[i], spy_train_slices[i], test_slices[i])\n",
    "    rmse_scores_mag7.append(rmse_mag7)\n",
    "    rmse_scores_spy.append(rmse_spy)\n",
    "    print(f\"Interval {i+1}: RMSE Mag7: {rmse_mag7}, RMSE SPY: {rmse_spy}\")\n",
    "\n",
    "# Output aggregated RMSE results\n",
    "print(\"Aggregated RMSE scores for Mag7 training:\", np.mean(rmse_scores_mag7))\n",
    "print(\"Aggregated RMSE scores for SPY training:\", np.mean(rmse_scores_spy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp    open    close  volume   returns  Updated Volume  \\\n",
      "0  2023-09-01 13:30:00  243.31  242.020    5596 -0.005302       3174465.0   \n",
      "1  2023-09-01 13:40:00  241.65  241.685    1763  0.000145       1894580.0   \n",
      "2  2023-09-01 13:50:00  241.37  240.970    3062 -0.001657       1320054.0   \n",
      "3  2023-09-01 14:00:00  240.92  241.570    4360  0.002698       1762548.0   \n",
      "4  2023-09-01 14:10:00  242.26  241.475    2439 -0.003240       1400384.0   \n",
      "\n",
      "   Updated Returns  SPY Volume  SPY Returns  GOOG Volume  ...  AMZN Volume  \\\n",
      "0        -0.005356   3482566.0    -0.002196    1654453.0  ...    3763564.0   \n",
      "1         0.001114   2197647.0    -0.001482    1063642.0  ...    1862218.0   \n",
      "2         0.001431   1699129.0     0.001851     689390.0  ...    1405575.0   \n",
      "3         0.001626   2582852.0     0.000420     595974.0  ...    1312147.0   \n",
      "4         0.000634   1984999.0     0.000928     472404.0  ...    1351848.0   \n",
      "\n",
      "   AMZN Returns  META Volume  META Returns  TSLA Volume  TSLA Returns  \\\n",
      "0     -0.005358    1211129.0     -0.005712    8625166.0     -0.011168   \n",
      "1     -0.003245     727137.0     -0.005645    6038988.0     -0.006900   \n",
      "2      0.002098     499321.0      0.003683    5013238.0      0.000317   \n",
      "3     -0.000336     482262.0      0.002863    5410776.0      0.000515   \n",
      "4      0.003178     376491.0      0.003258    3785600.0      0.002156   \n",
      "\n",
      "   MSFT Volume  MSFT Returns  AAPL Volume  AAPL Returns  \n",
      "0    1092635.0     -0.006610    3174465.0     -0.005356  \n",
      "1     657704.0     -0.003616    1894580.0      0.001114  \n",
      "2     492591.0      0.003427    1320054.0      0.001431  \n",
      "3     455361.0      0.000547    1762548.0      0.001626  \n",
      "4     505270.0      0.002856    1400384.0      0.000634  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded file\n",
    "file_path = 'preprocessed_data/Bottom_20/Updated_MKTX_Data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(data.head())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62f575909710a04b23ad53bfbfcbdffae8e3a88b5ad1048e7da5f6fcba1b84f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('cse185')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
