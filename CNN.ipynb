{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as nn\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# for time stamps\n",
    "yf.pdr_override()\n",
    "\n",
    "# define the top 7 S&P 500 companies\n",
    "top_7_list = ['GOOG', 'NVDA', 'AMZN', 'META', 'TSLA', 'AAPL', 'MSFT']\n",
    "\n",
    "# set the date range\n",
    "end = datetime.now()\n",
    "start = datetime(2015, 4, 16)\n",
    "\n",
    "# download the data and concatenate into a single DataFrame\n",
    "top_7_dfs = [yf.download(stock, start, end).assign(Company=stock) for stock in top_7_list]\n",
    "top_7_df = pd.concat(top_7_dfs)\n",
    "\n",
    "spy_df = yf.download('SPY', start, end)\n",
    "spy_df['ETF'] = 'SPY' \n",
    "\n",
    "bottom_20_list = ['ETSY', 'FMC', 'FRT', 'GNRC', 'HAS', 'IVZ', 'MHK', 'MKTX', 'NCLH', 'PARA',\n",
    "                  'PNW', 'RHI', 'VFC', 'WHR', 'XRAY', 'ZION']\n",
    "\n",
    "# download data and concatenate into a single DataFrame\n",
    "bottom_20_dfs = [yf.download(stock, start, end).assign(Company=stock) for stock in bottom_20_list]\n",
    "bottom_20_df = pd.concat(bottom_20_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neil\\AppData\\Local\\Temp\\ipykernel_4592\\2805420630.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  top_7_dfs[0]['Company'][0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GOOG'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_7_dfs[0]['Company'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [[spy_df], bottom_20_dfs, top_7_dfs]:\n",
    "    for dataset in l:\n",
    "        dataset['Returns'] = (1 - dataset['Close']/dataset['Open']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Company</th>\n",
       "      <th>Returns</th>\n",
       "      <th>SPY_Volume</th>\n",
       "      <th>SPY_Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-16</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.740002</td>\n",
       "      <td>28.219999</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>19763300</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>68934900</td>\n",
       "      <td>-0.161880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-17</th>\n",
       "      <td>29.770000</td>\n",
       "      <td>30.299999</td>\n",
       "      <td>26.510000</td>\n",
       "      <td>27.580000</td>\n",
       "      <td>27.580000</td>\n",
       "      <td>3965500</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>7.356401</td>\n",
       "      <td>191113200</td>\n",
       "      <td>0.473823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-20</th>\n",
       "      <td>28.770000</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>3076200</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>13.451515</td>\n",
       "      <td>92189500</td>\n",
       "      <td>-0.377886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-21</th>\n",
       "      <td>24.969999</td>\n",
       "      <td>26.040001</td>\n",
       "      <td>24.559999</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>2184700</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>-3.123751</td>\n",
       "      <td>72559800</td>\n",
       "      <td>0.507900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-22</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.240000</td>\n",
       "      <td>24.950001</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>1442500</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>3.384612</td>\n",
       "      <td>78264600</td>\n",
       "      <td>-0.295229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-11</th>\n",
       "      <td>42.270000</td>\n",
       "      <td>42.439999</td>\n",
       "      <td>41.240002</td>\n",
       "      <td>41.610001</td>\n",
       "      <td>41.610001</td>\n",
       "      <td>2607800</td>\n",
       "      <td>ZION</td>\n",
       "      <td>1.561391</td>\n",
       "      <td>62557200</td>\n",
       "      <td>-0.156713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-12</th>\n",
       "      <td>41.630001</td>\n",
       "      <td>42.040001</td>\n",
       "      <td>40.669998</td>\n",
       "      <td>40.900002</td>\n",
       "      <td>40.900002</td>\n",
       "      <td>2254900</td>\n",
       "      <td>ZION</td>\n",
       "      <td>1.753542</td>\n",
       "      <td>73114400</td>\n",
       "      <td>-0.648557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-13</th>\n",
       "      <td>40.650002</td>\n",
       "      <td>41.369999</td>\n",
       "      <td>40.570000</td>\n",
       "      <td>41.070000</td>\n",
       "      <td>41.070000</td>\n",
       "      <td>1956600</td>\n",
       "      <td>ZION</td>\n",
       "      <td>-1.033206</td>\n",
       "      <td>55104100</td>\n",
       "      <td>0.220459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-14</th>\n",
       "      <td>40.830002</td>\n",
       "      <td>41.049999</td>\n",
       "      <td>39.230000</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>2837400</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2.522661</td>\n",
       "      <td>110171800</td>\n",
       "      <td>0.390730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-15</th>\n",
       "      <td>39.599998</td>\n",
       "      <td>40.330002</td>\n",
       "      <td>39.380001</td>\n",
       "      <td>39.790001</td>\n",
       "      <td>39.790001</td>\n",
       "      <td>37419300</td>\n",
       "      <td>ZION</td>\n",
       "      <td>-0.479804</td>\n",
       "      <td>107585800</td>\n",
       "      <td>0.074480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35920 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
       "Date                                                                          \n",
       "2015-04-16  31.000000  35.740002  28.219999  30.000000  30.000000  19763300   \n",
       "2015-04-17  29.770000  30.299999  26.510000  27.580000  27.580000   3965500   \n",
       "2015-04-20  28.770000  28.900000  24.870001  24.900000  24.900000   3076200   \n",
       "2015-04-21  24.969999  26.040001  24.559999  25.750000  25.750000   2184700   \n",
       "2015-04-22  26.000000  26.240000  24.950001  25.120001  25.120001   1442500   \n",
       "...               ...        ...        ...        ...        ...       ...   \n",
       "2024-03-11  42.270000  42.439999  41.240002  41.610001  41.610001   2607800   \n",
       "2024-03-12  41.630001  42.040001  40.669998  40.900002  40.900002   2254900   \n",
       "2024-03-13  40.650002  41.369999  40.570000  41.070000  41.070000   1956600   \n",
       "2024-03-14  40.830002  41.049999  39.230000  39.799999  39.799999   2837400   \n",
       "2024-03-15  39.599998  40.330002  39.380001  39.790001  39.790001  37419300   \n",
       "\n",
       "           Company    Returns  SPY_Volume  SPY_Returns  \n",
       "Date                                                    \n",
       "2015-04-16    ETSY   3.225806    68934900    -0.161880  \n",
       "2015-04-17    ETSY   7.356401   191113200     0.473823  \n",
       "2015-04-20    ETSY  13.451515    92189500    -0.377886  \n",
       "2015-04-21    ETSY  -3.123751    72559800     0.507900  \n",
       "2015-04-22    ETSY   3.384612    78264600    -0.295229  \n",
       "...            ...        ...         ...          ...  \n",
       "2024-03-11    ZION   1.561391    62557200    -0.156713  \n",
       "2024-03-12    ZION   1.753542    73114400    -0.648557  \n",
       "2024-03-13    ZION  -1.033206    55104100     0.220459  \n",
       "2024-03-14    ZION   2.522661   110171800     0.390730  \n",
       "2024-03-15    ZION  -0.479804   107585800     0.074480  \n",
       "\n",
       "[35920 rows x 10 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy_main = pd.DataFrame()\n",
    "spy_temp = spy_df[['Volume', 'Returns']]\n",
    "spy_columns = spy_temp.rename(columns = {'Volume': 'SPY_Volume', 'Returns': 'SPY_Returns'})\n",
    "for dataset in bottom_20_dfs:\n",
    "    temp_df = pd.concat([dataset, spy_columns], axis=1)\n",
    "    spy_main = pd.concat([spy_main, temp_df])\n",
    "spy_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Company</th>\n",
       "      <th>Returns</th>\n",
       "      <th>GOOG_Volume</th>\n",
       "      <th>GOOG_Returns</th>\n",
       "      <th>...</th>\n",
       "      <th>AMZN_Volume</th>\n",
       "      <th>AMZN_Returns</th>\n",
       "      <th>META_Volume</th>\n",
       "      <th>META_Returns</th>\n",
       "      <th>TSLA_Volume</th>\n",
       "      <th>TSLA_Returns</th>\n",
       "      <th>AAPL_Volume</th>\n",
       "      <th>AAPL_Returns</th>\n",
       "      <th>MSFT_Volume</th>\n",
       "      <th>MSFT_Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-16</th>\n",
       "      <td>31.0</td>\n",
       "      <td>35.740002</td>\n",
       "      <td>28.219999</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19763300</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>25997180</td>\n",
       "      <td>-0.735986</td>\n",
       "      <td>...</td>\n",
       "      <td>41608000</td>\n",
       "      <td>-0.612475</td>\n",
       "      <td>13769700</td>\n",
       "      <td>0.194014</td>\n",
       "      <td>24886500</td>\n",
       "      <td>0.48147</td>\n",
       "      <td>113476000</td>\n",
       "      <td>0.087108</td>\n",
       "      <td>22509700</td>\n",
       "      <td>-0.500594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-17</th>\n",
       "      <td>29.77</td>\n",
       "      <td>30.299999</td>\n",
       "      <td>26.51</td>\n",
       "      <td>27.58</td>\n",
       "      <td>27.58</td>\n",
       "      <td>3965500</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>7.356401</td>\n",
       "      <td>43037837</td>\n",
       "      <td>0.872012</td>\n",
       "      <td>...</td>\n",
       "      <td>76794000</td>\n",
       "      <td>1.84774</td>\n",
       "      <td>24215000</td>\n",
       "      <td>0.859112</td>\n",
       "      <td>37048500</td>\n",
       "      <td>-0.878091</td>\n",
       "      <td>207828000</td>\n",
       "      <td>0.637199</td>\n",
       "      <td>42387600</td>\n",
       "      <td>0.119989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-20</th>\n",
       "      <td>28.77</td>\n",
       "      <td>28.9</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.9</td>\n",
       "      <td>24.9</td>\n",
       "      <td>3076200</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>13.451515</td>\n",
       "      <td>33585958</td>\n",
       "      <td>-1.860731</td>\n",
       "      <td>...</td>\n",
       "      <td>100322000</td>\n",
       "      <td>-2.89526</td>\n",
       "      <td>28796800</td>\n",
       "      <td>-1.900902</td>\n",
       "      <td>38389500</td>\n",
       "      <td>0.73024</td>\n",
       "      <td>188217200</td>\n",
       "      <td>-1.616627</td>\n",
       "      <td>46057700</td>\n",
       "      <td>-2.827703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-21</th>\n",
       "      <td>24.969999</td>\n",
       "      <td>26.040001</td>\n",
       "      <td>24.559999</td>\n",
       "      <td>25.75</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2184700</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>-3.123751</td>\n",
       "      <td>36895018</td>\n",
       "      <td>0.658586</td>\n",
       "      <td>...</td>\n",
       "      <td>92870000</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>27171900</td>\n",
       "      <td>0.452378</td>\n",
       "      <td>51487500</td>\n",
       "      <td>-1.754128</td>\n",
       "      <td>129740400</td>\n",
       "      <td>0.928964</td>\n",
       "      <td>26013800</td>\n",
       "      <td>0.837211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-22</th>\n",
       "      <td>26.0</td>\n",
       "      <td>26.24</td>\n",
       "      <td>24.950001</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>1442500</td>\n",
       "      <td>ETSY</td>\n",
       "      <td>3.384612</td>\n",
       "      <td>31871263</td>\n",
       "      <td>-0.929078</td>\n",
       "      <td>...</td>\n",
       "      <td>69494000</td>\n",
       "      <td>0.538395</td>\n",
       "      <td>45548000</td>\n",
       "      <td>-0.367644</td>\n",
       "      <td>117945000</td>\n",
       "      <td>-3.265874</td>\n",
       "      <td>150618000</td>\n",
       "      <td>-1.283563</td>\n",
       "      <td>25064300</td>\n",
       "      <td>-0.74995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-11</th>\n",
       "      <td>42.27</td>\n",
       "      <td>42.439999</td>\n",
       "      <td>41.240002</td>\n",
       "      <td>41.610001</td>\n",
       "      <td>41.610001</td>\n",
       "      <td>2607800</td>\n",
       "      <td>ZION</td>\n",
       "      <td>1.561391</td>\n",
       "      <td>22536400</td>\n",
       "      <td>-1.364263</td>\n",
       "      <td>...</td>\n",
       "      <td>28484800</td>\n",
       "      <td>1.348168</td>\n",
       "      <td>20428300</td>\n",
       "      <td>2.70015</td>\n",
       "      <td>85391500</td>\n",
       "      <td>-1.322318</td>\n",
       "      <td>60139500</td>\n",
       "      <td>0.109866</td>\n",
       "      <td>16120800</td>\n",
       "      <td>-0.188225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-12</th>\n",
       "      <td>41.630001</td>\n",
       "      <td>42.040001</td>\n",
       "      <td>40.669998</td>\n",
       "      <td>40.900002</td>\n",
       "      <td>40.900002</td>\n",
       "      <td>2254900</td>\n",
       "      <td>ZION</td>\n",
       "      <td>1.753542</td>\n",
       "      <td>19019700</td>\n",
       "      <td>-0.990955</td>\n",
       "      <td>...</td>\n",
       "      <td>36610600</td>\n",
       "      <td>-1.089337</td>\n",
       "      <td>15448200</td>\n",
       "      <td>-1.315734</td>\n",
       "      <td>87391700</td>\n",
       "      <td>0.129387</td>\n",
       "      <td>59825400</td>\n",
       "      <td>-0.046204</td>\n",
       "      <td>22457000</td>\n",
       "      <td>-1.879202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-13</th>\n",
       "      <td>40.650002</td>\n",
       "      <td>41.369999</td>\n",
       "      <td>40.57</td>\n",
       "      <td>41.07</td>\n",
       "      <td>41.07</td>\n",
       "      <td>1956600</td>\n",
       "      <td>ZION</td>\n",
       "      <td>-1.033206</td>\n",
       "      <td>19637000</td>\n",
       "      <td>-0.50693</td>\n",
       "      <td>...</td>\n",
       "      <td>30772600</td>\n",
       "      <td>-0.375215</td>\n",
       "      <td>12090700</td>\n",
       "      <td>-0.036334</td>\n",
       "      <td>106524500</td>\n",
       "      <td>2.062992</td>\n",
       "      <td>52488700</td>\n",
       "      <td>0.949238</td>\n",
       "      <td>17115900</td>\n",
       "      <td>0.717532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-14</th>\n",
       "      <td>40.830002</td>\n",
       "      <td>41.049999</td>\n",
       "      <td>39.23</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>2837400</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2.522661</td>\n",
       "      <td>36117900</td>\n",
       "      <td>-1.433586</td>\n",
       "      <td>...</td>\n",
       "      <td>43705800</td>\n",
       "      <td>-0.596543</td>\n",
       "      <td>12620000</td>\n",
       "      <td>1.685128</td>\n",
       "      <td>126325700</td>\n",
       "      <td>3.141208</td>\n",
       "      <td>72913500</td>\n",
       "      <td>-0.052048</td>\n",
       "      <td>34157300</td>\n",
       "      <td>-1.18504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-15</th>\n",
       "      <td>39.599998</td>\n",
       "      <td>40.330002</td>\n",
       "      <td>39.380001</td>\n",
       "      <td>39.790001</td>\n",
       "      <td>39.790001</td>\n",
       "      <td>37419300</td>\n",
       "      <td>ZION</td>\n",
       "      <td>-0.479804</td>\n",
       "      <td>41025900</td>\n",
       "      <td>0.864658</td>\n",
       "      <td>...</td>\n",
       "      <td>72115500</td>\n",
       "      <td>1.256794</td>\n",
       "      <td>29141700</td>\n",
       "      <td>1.00407</td>\n",
       "      <td>96971900</td>\n",
       "      <td>-0.251289</td>\n",
       "      <td>121664700</td>\n",
       "      <td>-0.847109</td>\n",
       "      <td>45049800</td>\n",
       "      <td>0.684489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35920 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close    Volume  \\\n",
       "Date                                                                          \n",
       "2015-04-16       31.0  35.740002  28.219999       30.0       30.0  19763300   \n",
       "2015-04-17      29.77  30.299999      26.51      27.58      27.58   3965500   \n",
       "2015-04-20      28.77       28.9  24.870001       24.9       24.9   3076200   \n",
       "2015-04-21  24.969999  26.040001  24.559999      25.75      25.75   2184700   \n",
       "2015-04-22       26.0      26.24  24.950001  25.120001  25.120001   1442500   \n",
       "...               ...        ...        ...        ...        ...       ...   \n",
       "2024-03-11      42.27  42.439999  41.240002  41.610001  41.610001   2607800   \n",
       "2024-03-12  41.630001  42.040001  40.669998  40.900002  40.900002   2254900   \n",
       "2024-03-13  40.650002  41.369999      40.57      41.07      41.07   1956600   \n",
       "2024-03-14  40.830002  41.049999      39.23  39.799999  39.799999   2837400   \n",
       "2024-03-15  39.599998  40.330002  39.380001  39.790001  39.790001  37419300   \n",
       "\n",
       "           Company    Returns GOOG_Volume GOOG_Returns  ... AMZN_Volume  \\\n",
       "Date                                                    ...               \n",
       "2015-04-16    ETSY   3.225806    25997180    -0.735986  ...    41608000   \n",
       "2015-04-17    ETSY   7.356401    43037837     0.872012  ...    76794000   \n",
       "2015-04-20    ETSY  13.451515    33585958    -1.860731  ...   100322000   \n",
       "2015-04-21    ETSY  -3.123751    36895018     0.658586  ...    92870000   \n",
       "2015-04-22    ETSY   3.384612    31871263    -0.929078  ...    69494000   \n",
       "...            ...        ...         ...          ...  ...         ...   \n",
       "2024-03-11    ZION   1.561391    22536400    -1.364263  ...    28484800   \n",
       "2024-03-12    ZION   1.753542    19019700    -0.990955  ...    36610600   \n",
       "2024-03-13    ZION  -1.033206    19637000     -0.50693  ...    30772600   \n",
       "2024-03-14    ZION   2.522661    36117900    -1.433586  ...    43705800   \n",
       "2024-03-15    ZION  -0.479804    41025900     0.864658  ...    72115500   \n",
       "\n",
       "           AMZN_Returns META_Volume META_Returns TSLA_Volume TSLA_Returns  \\\n",
       "Date                                                                        \n",
       "2015-04-16    -0.612475    13769700     0.194014    24886500      0.48147   \n",
       "2015-04-17      1.84774    24215000     0.859112    37048500    -0.878091   \n",
       "2015-04-20     -2.89526    28796800    -1.900902    38389500      0.73024   \n",
       "2015-04-21     0.033223    27171900     0.452378    51487500    -1.754128   \n",
       "2015-04-22     0.538395    45548000    -0.367644   117945000    -3.265874   \n",
       "...                 ...         ...          ...         ...          ...   \n",
       "2024-03-11     1.348168    20428300      2.70015    85391500    -1.322318   \n",
       "2024-03-12    -1.089337    15448200    -1.315734    87391700     0.129387   \n",
       "2024-03-13    -0.375215    12090700    -0.036334   106524500     2.062992   \n",
       "2024-03-14    -0.596543    12620000     1.685128   126325700     3.141208   \n",
       "2024-03-15     1.256794    29141700      1.00407    96971900    -0.251289   \n",
       "\n",
       "           AAPL_Volume AAPL_Returns MSFT_Volume MSFT_Returns  \n",
       "Date                                                          \n",
       "2015-04-16   113476000     0.087108    22509700    -0.500594  \n",
       "2015-04-17   207828000     0.637199    42387600     0.119989  \n",
       "2015-04-20   188217200    -1.616627    46057700    -2.827703  \n",
       "2015-04-21   129740400     0.928964    26013800     0.837211  \n",
       "2015-04-22   150618000    -1.283563    25064300     -0.74995  \n",
       "...                ...          ...         ...          ...  \n",
       "2024-03-11    60139500     0.109866    16120800    -0.188225  \n",
       "2024-03-12    59825400    -0.046204    22457000    -1.879202  \n",
       "2024-03-13    52488700     0.949238    17115900     0.717532  \n",
       "2024-03-14    72913500    -0.052048    34157300     -1.18504  \n",
       "2024-03-15   121664700    -0.847109    45049800     0.684489  \n",
       "\n",
       "[35920 rows x 22 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mag7_main_T = pd.DataFrame()\n",
    "temp_main = pd.DataFrame()\n",
    "\n",
    "for company in top_7_dfs:\n",
    "    company_temp = company[['Volume', 'Returns']]\n",
    "    spy_columns = company_temp.rename(columns = {'Volume': f\"{str(company['Company'][0])}\" + '_Volume', 'Returns': f\"{str(company['Company'][0])}\" + '_Returns'})\n",
    "    for dataset in bottom_20_dfs:\n",
    "        temp_df = pd.concat([dataset, spy_columns], axis=1)\n",
    "        temp_main = pd.concat([temp_main, temp_df])\n",
    "    mag7_main_T = pd.concat([mag7_main_T, temp_main], axis=1)\n",
    "    temp_main = pd.DataFrame()\n",
    "\n",
    "mag7_main = mag7_main_T.T\n",
    "mag7_main = mag7_main.drop_duplicates().T\n",
    "mag7_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_main = pd.get_dummies(spy_main, columns=['Company'])\n",
    "mag7_main = pd.get_dummies(mag7_main, columns=['Company'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(dataframe):\n",
    "    X_chunks = []\n",
    "    Y_chunks = []\n",
    "    \n",
    "    num_rows = len(dataframe)\n",
    "    start = 0\n",
    "    \n",
    "    # List of company columns\n",
    "    company_columns = [col for col in dataframe.columns if col.startswith('Company_')]\n",
    "    \n",
    "    while start < num_rows - 2:\n",
    "        if (dataframe.index[start + 1] - dataframe.index[start]).days == 1 and \\\n",
    "           (dataframe.index[start + 2] - dataframe.index[start + 1]).days == 1:\n",
    "            \n",
    "            # Check if the same company column is True across all three days\n",
    "            if dataframe.iloc[start:start+3][company_columns].sum(axis=0).max() == 3:\n",
    "                # The chunk pertains to the same company\n",
    "                X_chunk = dataframe.iloc[start:start+2, :]\n",
    "                Y_chunk = dataframe.iloc[start + 2]['Returns']\n",
    "                \n",
    "                X_chunks.append(X_chunk)\n",
    "                Y_chunks.append(Y_chunk)\n",
    "                \n",
    "                start += 1\n",
    "            else:\n",
    "                # The chunk does not pertain to the same company, skip to the next possible start\n",
    "                start += 1\n",
    "        else:\n",
    "            # Dates are not consecutive, skip to the next possible start\n",
    "            start += 1\n",
    "    \n",
    "    return X_chunks, Y_chunks\n",
    "\n",
    "X_spy, y_spy = create_chunks(spy_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Returns</th>\n",
       "      <th>SPY_Volume</th>\n",
       "      <th>SPY_Returns</th>\n",
       "      <th>Company_ETSY</th>\n",
       "      <th>...</th>\n",
       "      <th>Company_MHK</th>\n",
       "      <th>Company_MKTX</th>\n",
       "      <th>Company_NCLH</th>\n",
       "      <th>Company_PARA</th>\n",
       "      <th>Company_PNW</th>\n",
       "      <th>Company_RHI</th>\n",
       "      <th>Company_VFC</th>\n",
       "      <th>Company_WHR</th>\n",
       "      <th>Company_XRAY</th>\n",
       "      <th>Company_ZION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-20</th>\n",
       "      <td>28.770000</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>24.870001</td>\n",
       "      <td>24.90</td>\n",
       "      <td>24.90</td>\n",
       "      <td>3076200</td>\n",
       "      <td>13.451515</td>\n",
       "      <td>92189500</td>\n",
       "      <td>-0.377886</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-21</th>\n",
       "      <td>24.969999</td>\n",
       "      <td>26.040001</td>\n",
       "      <td>24.559999</td>\n",
       "      <td>25.75</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2184700</td>\n",
       "      <td>-3.123751</td>\n",
       "      <td>72559800</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low  Close  Adj Close   Volume  \\\n",
       "Date                                                                     \n",
       "2015-04-20  28.770000  28.900000  24.870001  24.90      24.90  3076200   \n",
       "2015-04-21  24.969999  26.040001  24.559999  25.75      25.75  2184700   \n",
       "\n",
       "              Returns  SPY_Volume  SPY_Returns  Company_ETSY  ...  \\\n",
       "Date                                                          ...   \n",
       "2015-04-20  13.451515    92189500    -0.377886          True  ...   \n",
       "2015-04-21  -3.123751    72559800     0.507900          True  ...   \n",
       "\n",
       "            Company_MHK  Company_MKTX  Company_NCLH  Company_PARA  \\\n",
       "Date                                                                \n",
       "2015-04-20        False         False         False         False   \n",
       "2015-04-21        False         False         False         False   \n",
       "\n",
       "            Company_PNW  Company_RHI  Company_VFC  Company_WHR  Company_XRAY  \\\n",
       "Date                                                                           \n",
       "2015-04-20        False        False        False        False         False   \n",
       "2015-04-21        False        False        False        False         False   \n",
       "\n",
       "            Company_ZION  \n",
       "Date                      \n",
       "2015-04-20         False  \n",
       "2015-04-21         False  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_spy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3846121567946197"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_spy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_spy, y_spy, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(X_data):\n",
    "    processed_arrays = []\n",
    "    for X in X_data:\n",
    "        X_arr = X.to_numpy(dtype=np.float32)\n",
    "        X_reshaped = X_arr.reshape(1, 1, 2, X.shape[1])\n",
    "        processed_arrays.append(X_reshaped)\n",
    "    concatenated_array = np.concatenate(processed_arrays, axis=0)\n",
    "    processed_tensor = torch.tensor(concatenated_array, dtype=torch.float32)\n",
    "    \n",
    "    return processed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16524, 1, 2, 25])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_spy_train = PreProcess(X_train)\n",
    "X_spy_test = PreProcess(X_test)\n",
    "X_spy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0307e+02,  2.1192e+02,  1.9100e+02,  2.1075e+02,  2.1075e+02,\n",
       "           3.6461e+06, -3.7819e+00,  1.0728e+08, -7.3831e-01,  1.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 2.0999e+02,  2.1288e+02,  2.0226e+02,  2.0910e+02,  2.0910e+02,\n",
       "           2.1790e+06,  4.2383e-01,  7.2434e+07, -1.4081e+00,  1.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_spy_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16524])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_spy_train = torch.tensor([y_train], dtype=torch.float32)\n",
    "y_spy_train = y_spy_train.view(-1)\n",
    "y_spy_test = torch.tensor([y_test], dtype=torch.float32)\n",
    "y_spy_test = y_spy_test.view(-1)\n",
    "\n",
    "y_spy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "num_classes = 1\n",
    "num_epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_spy_train, y_spy_train)\n",
    "test_dataset = TensorDataset(X_spy_test, y_spy_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.3120e+01,  4.3350e+01,  4.2870e+01,  4.2960e+01,  3.5095e+01,\n",
       "            2.5081e+06,  3.7106e-01,  5.6906e+07,  5.0768e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 4.3030e+01,  4.3230e+01,  4.2250e+01,  4.2330e+01,  3.4580e+01,\n",
       "            2.1235e+06,  1.6268e+00,  5.5978e+07,  2.0948e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 8.8840e+01,  8.8840e+01,  8.7290e+01,  8.7990e+01,  7.4890e+01,\n",
       "            1.6637e+06,  9.5677e-01,  7.5768e+07,  9.3772e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 8.7070e+01,  8.7070e+01,  8.5490e+01,  8.5990e+01,  7.3188e+01,\n",
       "            2.0004e+06,  1.2404e+00,  7.3942e+07, -2.9159e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4500e+02,  1.4921e+02,  1.4144e+02,  1.4247e+02,  1.4247e+02,\n",
       "            2.5029e+06,  1.7448e+00,  7.9426e+07,  6.9589e-01,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 1.4324e+02,  1.4397e+02,  1.3660e+02,  1.4096e+02,  1.4096e+02,\n",
       "            2.0449e+06,  1.5917e+00,  6.4737e+07, -1.0203e+00,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 3.1180e+01,  3.1240e+01,  3.0480e+01,  3.0840e+01,  3.0840e+01,\n",
       "            1.0296e+07,  1.0904e+00,  5.7700e+07, -5.3700e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 3.0820e+01,  3.1380e+01,  3.0780e+01,  3.1000e+01,  3.1000e+01,\n",
       "            8.1631e+06, -5.8404e-01,  4.9445e+07,  1.3942e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.8200e+01,  4.8800e+01,  4.8080e+01,  4.8600e+01,  4.1814e+01,\n",
       "            1.0131e+06, -8.2987e-01,  8.8946e+07, -4.1186e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 4.8380e+01,  4.8610e+01,  4.7770e+01,  4.8250e+01,  4.1719e+01,\n",
       "            1.0382e+06,  2.6871e-01,  6.2115e+07, -1.1016e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 5.1320e+01,  5.1610e+01,  4.8680e+01,  4.9220e+01,  4.6010e+01,\n",
       "            2.0495e+06,  4.0920e+00,  7.9747e+07,  1.3279e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 4.9180e+01,  4.9640e+01,  4.6680e+01,  4.7130e+01,  4.4056e+01,\n",
       "            2.3012e+06,  4.1684e+00,  8.8283e+07,  7.1190e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 9.3070e+01,  9.5020e+01,  9.2830e+01,  9.5010e+01,  8.9603e+01,\n",
       "            9.4700e+05, -2.0845e+00,  6.7397e+07,  2.0459e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 9.5290e+01,  9.6390e+01,  9.4520e+01,  9.5670e+01,  9.0225e+01,\n",
       "            9.6550e+05, -3.9878e-01,  5.2472e+07,  1.9332e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.6900e+02,  4.7381e+02,  4.6687e+02,  4.7214e+02,  4.5969e+02,\n",
       "            1.9400e+05, -6.6951e-01,  3.5970e+07,  4.2066e-02,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 4.7336e+02,  4.7336e+02,  4.6135e+02,  4.6359e+02,  4.5136e+02,\n",
       "            2.7660e+05,  2.0640e+00,  6.4828e+07, -1.9897e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6540e+01,  1.7080e+01,  1.6260e+01,  1.6970e+01,  1.6970e+01,\n",
       "            1.7407e+07, -2.5997e+00,  8.3030e+07, -4.4031e-01,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 1.6580e+01,  1.6880e+01,  1.5510e+01,  1.5900e+01,  1.5900e+01,\n",
       "            2.5812e+07,  4.1013e+00,  1.1767e+08,  2.8848e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3600e+02,  1.3677e+02,  1.3515e+02,  1.3645e+02,  1.0031e+02,\n",
       "            6.5210e+05, -3.3088e-01,  1.6345e+08, -6.6187e-01,  0.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 1.3679e+02,  1.3721e+02,  1.3398e+02,  1.3560e+02,  9.9682e+01,\n",
       "            6.8470e+05,  8.6994e-01,  1.3108e+08, -2.6032e-02,  0.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = iter(train_loader)\n",
    "next(ex)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(2, 5), stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2, 3), stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, 3), stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1, 3), stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(1, 3), stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Adjusted for additional conv layers. Dynamically calculate in_features in forward().\n",
    "        self.fc1 = nn.Linear(in_features=512, out_features=512)  # Placeholder, will adjust dynamically\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Before flattening, dynamically adjust fc1's in_features based on the conv5 output\n",
    "        n_size = x.size()[1] * x.size()[2] * x.size()[3]\n",
    "        self.fc1 = nn.Linear(in_features=n_size, out_features=512).to(x.device)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCNN().to(device)\n",
    "\n",
    "def custom_sign_loss(output, target):\n",
    "    sign_agreement = ((output * target) >= 0).float()  # 1 when signs agree, 0 otherwise\n",
    "    magnitude_loss = torch.abs(output - target)  # or any other magnitude-based loss\n",
    "    return torch.mean((1 - sign_agreement) * magnitude_loss)\n",
    "\n",
    "def custom_hinge_loss(outputs, targets):\n",
    "    targets = targets.view_as(outputs)\n",
    "    loss = torch.where(targets * outputs < 0, torch.abs(outputs), torch.tensor(0.0, device=outputs.device))\n",
    "    return torch.mean(loss)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate is: 0.001\n",
      "Epoch 1/100 - Training Loss: 4.2906, Validation Loss: 4.2092, Val_Accuracy: 0.4954, Time: 43.51 sec\n",
      "Epoch 2/100 - Training Loss: 4.2794, Validation Loss: 4.2305, Val_Accuracy: 0.4993, Time: 43.66 sec\n",
      "Epoch 3/100 - Training Loss: 4.2770, Validation Loss: 4.2092, Val_Accuracy: 0.4978, Time: 43.92 sec\n",
      "Epoch 4/100 - Training Loss: 4.2745, Validation Loss: 4.2045, Val_Accuracy: 0.4969, Time: 45.92 sec\n",
      "Epoch 5/100 - Training Loss: 4.2761, Validation Loss: 4.2074, Val_Accuracy: 0.4971, Time: 44.27 sec\n",
      "Epoch 6/100 - Training Loss: 4.2756, Validation Loss: 4.2090, Val_Accuracy: 0.5041, Time: 44.42 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[288], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[286], line 39\u001b[0m, in \u001b[0;36mMyCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Before flattening, dynamically adjust fc1's in_features based on the conv5 output\u001b[39;00m\n\u001b[0;32m     38\u001b[0m n_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:109\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\init.py:459\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[0;32m    457\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_min = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    loss_train = running_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct_signs = 0  # Counter for predictions with the correct sign\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets.view(-1, 1))\n",
    "            val_loss += loss.item() * inputs.size(0)  # Multiply by batch size\n",
    "            \n",
    "            # Calculate the accuracy based on the sign\n",
    "            predicted_signs = torch.sign(outputs)\n",
    "            target_signs = torch.sign(targets.view(-1, 1))\n",
    "            correct_signs += torch.sum(predicted_signs == target_signs).item()\n",
    "            if correct_signs > 100 and epoch > 25:\n",
    "                print(targets, outputs)\n",
    "            total_samples += inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(test_loader.dataset)\n",
    "        val_acc = correct_signs / total_samples\n",
    "    \n",
    "    epoch_end_time = time.time()  # End time of the epoch\n",
    "    epoch_duration = epoch_end_time - start_time  # Calculate duration of the epoch\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Current learning rate is: {}\".format(param_group['lr']))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {loss_train:.4f}, Validation Loss: {val_loss:.4f}, Val_Accuracy: {val_acc:.4f}, Time: {epoch_duration:.2f} sec\")\n",
    "\n",
    "    if val_loss < loss_min:\n",
    "        loss_min = val_loss\n",
    "        torch.save(model.state_dict(), './CNN_SPY_Model.pth')\n",
    "        print(f'\\n Minimum Val Loss of {val_loss:.4f} at epoch {epoch+1}')\n",
    "        print(f'Model Saved \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mag 7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mag, y_mag = create_chunks(mag7_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16524, 1, 2, 37])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mag, y_mag, test_size=0.2, random_state=42)\n",
    "\n",
    "X_spy_train = PreProcess(X_train)\n",
    "X_spy_test = PreProcess(X_test)\n",
    "\n",
    "y_spy_train = torch.tensor([y_train], dtype=torch.float32)\n",
    "y_spy_train = y_spy_train.view(-1)\n",
    "y_spy_test = torch.tensor([y_test], dtype=torch.float32)\n",
    "y_spy_test = y_spy_test.view(-1)\n",
    "\n",
    "train_dataset = TensorDataset(X_spy_train, y_spy_train)\n",
    "test_dataset = TensorDataset(X_spy_test, y_spy_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_spy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate is: 0.001\n",
      "Epoch 1/100 - Training Loss: 4.2751, Validation Loss: 4.2087, Val_Accuracy: 0.4923, Time: 120.11 sec\n",
      "\n",
      " Minimum Val Loss of 4.2087 at epoch 1\n",
      "Model Saved \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[296], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_min = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    loss_train = running_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct_signs = 0  # Counter for predictions with the correct sign\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets.view(-1, 1))\n",
    "            val_loss += loss.item() * inputs.size(0)  # Multiply by batch size\n",
    "            \n",
    "            # Calculate the accuracy based on the sign\n",
    "            predicted_signs = torch.sign(outputs)\n",
    "            target_signs = torch.sign(targets.view(-1, 1))\n",
    "            correct_signs += torch.sum(predicted_signs == target_signs).item()\n",
    "            if correct_signs > 100 and epoch > 25:\n",
    "                print(targets, outputs)\n",
    "            total_samples += inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(test_loader.dataset)\n",
    "        val_acc = correct_signs / total_samples\n",
    "    \n",
    "    epoch_end_time = time.time()  # End time of the epoch\n",
    "    epoch_duration = epoch_end_time - start_time  # Calculate duration of the epoch\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Current learning rate is: {}\".format(param_group['lr']))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {loss_train:.4f}, Validation Loss: {val_loss:.4f}, Val_Accuracy: {val_acc:.4f}, Time: {epoch_duration:.2f} sec\")\n",
    "\n",
    "    if val_loss < loss_min:\n",
    "        loss_min = val_loss\n",
    "        torch.save(model.state_dict(), './CNN_Mag7_Model.pth')\n",
    "        print(f'\\n Minimum Val Loss of {val_loss:.4f} at epoch {epoch+1}')\n",
    "        print(f'Model Saved \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
